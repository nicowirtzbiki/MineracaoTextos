{
 "cells": [
  {
   "attachments": {
    "Unifor_logo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Unifor_logo.png](attachment:Unifor_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINERAÇÃO DE TEXTOS E DA WEB\n",
    "\n",
    "## ATIVIDADE 02\n",
    "\n",
    "#### GRUPO 02:\n",
    "- Agenor Júnior\n",
    "- Nicole Wirtzbiki\n",
    "- Torricelli Evangelista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "INICIO ATIVIDADE 02\n",
    "\n",
    "- Extração de BOW-unigrama e bigrama dos tweets\n",
    "- Consulta a bases de conhecimento externas (WordNet, Empath) para definir sinônimos, hiperônimos ou palavras relacionadas com conteúdo ofensivo.\n",
    "- Vetorização dos tweets usando TF_IDF para os unigramas e bigramas\n",
    "- Recuperação dos word-embeddings das palavras de cada tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANTO PACOTES NECESSÁRIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('average_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "language = \"english\"\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "import os\n",
    "#with redirect_stdout(open(os.devnull, \"w\")):\n",
    "    #nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitura para objeto dataframe\n",
    "tweets = pd.read_csv('/home/nico/Área de Trabalho/MineracaoDadosWeb/entrega 2/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8')\n",
    "\n",
    "#conversão da coluna 'id' de inteiro para string\n",
    "tweets['id'] = tweets['id'].astype('str')\n",
    "\n",
    "#visualização dos primeiros registros\n",
    "tweets = tweets[['subtask_c','subtask_b','subtask_a','id','tweet']]\n",
    "#tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Normalização\n",
    "\n",
    "entrada: tweet  |  saída: tweet_normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importando o módulo \"REGEX\" para expressões regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacao_texto(tweet):\n",
    "    \n",
    "    #Normalização de todas as palavras para caixa baixa\n",
    "    tweet = tweet.lower() \n",
    "    #remover repetições de \"@user\"\n",
    "    #tweet = re.sub('(@user |@user| @user )+',' @user ',tweet)\n",
    "    \n",
    "    #remove as menções a usuários de cada tweet\n",
    "    tweet = re.sub(r'@user', '', tweet, flags=re.MULTILINE)\n",
    "    #Remoção de repetições seguidas de acentuações\n",
    "    tweet = re.sub('(!)+','!',tweet)\n",
    "    tweet = re.sub('(\")+','\"',tweet)\n",
    "    tweet = re.sub('(\\.)+','.',tweet)\n",
    "    #Remoção de todas as palavras que começam com \"#\"\n",
    "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n",
    "    #remove as palavras url\n",
    "    tweet = re.sub(r'url', '', tweet, flags=re.MULTILINE)\n",
    "    #remove aspas e apóstofres\n",
    "    tweet = re.sub('[\\'\"‘’“”…]', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "tweets['tweet_normalizado'] = tweets['tweet'].apply(normalizacao_texto) \n",
    "#tweets[tweets.columns[::-1]].head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARANDO AS SENTENÇAS\n",
    "- entrada: tweet_normalizado | saída: tweet_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_sents'] = [sent_tokenize(z) for z in tweets['tweet_normalizado']]\n",
    "#tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARANDO AS PALAVRAS\n",
    "- entrada: tweet_sents | saída: tweet_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sents):\n",
    "    tokens = []\n",
    "    for word in sents:\n",
    "        tokens.append(nltk.word_tokenize(word)) #separando palavras\n",
    "    return tokens\n",
    "\n",
    "tweets['tweet_tokens'] = tweets['tweet_sents'].apply(tokenize)\n",
    "#tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Remoção de Stop Words \n",
    "entrada: tweet_tokens | saída: tweet_tokens_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sents_list):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for tokens_list in sents_list:\n",
    "        tokens = [x for x in tokens_list if not x in stop_words]\n",
    "        out_list.append(tokens)\n",
    "        \n",
    "    return out_list\n",
    "\n",
    "   \n",
    "tweets['tweet_tokens_sw'] = tweets['tweet_tokens'].apply(remove_stop_words)\n",
    "#tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - POS Tagger\n",
    "\n",
    "entrada: tweet_tokens_sw | saída: tweet_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(sent_tokens):\n",
    "    \n",
    "    sent_tagged = []\n",
    "    for token in sent_tokens:\n",
    "        sent_tagged.append(nltk.pos_tag(token))\n",
    "\n",
    "    return sent_tagged\n",
    "\n",
    "tweets['tweet_tagged'] = tweets['tweet_tokens_sw'].apply(pos_tagger)\n",
    "#tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Lemmatização\n",
    "\n",
    "entrada: tweet_tagged | saída: tweet_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAPEANDO OS POS-TAGS DO NLTK PARA O FORMATO ACEITO PELO WORDNET LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {\"J\": wn.ADJ,\n",
    "            \"N\": wn.NOUN,\n",
    "            \"V\": wn.VERB,\n",
    "            \"R\": wn.ADV}\n",
    "\n",
    "def extract_wnpostag_from_postag(tag):\n",
    "    \n",
    "    #pega a primeira letra da tag\n",
    "    #segundo parâmetro opcional caso haja chave ausente no dicionário \n",
    "    return tag_dict.get(tag[0].upper(), None)\n",
    "\n",
    "def lemmatize_tupla_word_postag(tupla):\n",
    "    \n",
    "    #retorna uma tupla na forma (wordString, posTagString) \n",
    "    #como ('guitar', 'NN'), retorna a palavra lematizada.\n",
    "    tag = extract_wnpostag_from_postag(tupla[1])    \n",
    "    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZANDO OS TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tweets(sent_list):\n",
    "    \n",
    "    out_list = []\n",
    "    for token in sent_list:\n",
    "        lemmas = [lemmatize_tupla_word_postag(x) for x in token]\n",
    "        out_list.append(lemmas)\n",
    "\n",
    "    return out_list\n",
    "\n",
    "tweets['tweet_lemma'] = tweets['tweet_tagged'].apply(lemmatize_tweets)\n",
    "#tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Chunking\n",
    "\n",
    "entrada: tweet_tagged | saída: tweet_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "#with redirect_stdout(open(os.devnull, \"w\")):\n",
    "    #nltk.download('maxent_ne_chunker')\n",
    "    #nltk.download('words')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "def chunker (tweets_list):\n",
    "    \n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "    pattern1 = 'NP: {<DT>?<JJ>*<NN.*>*}'\n",
    "    pattern2 = 'NP: {<DT><NN.*><.*>*<NN.*>}'\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for lista in tweets_list:\n",
    "        cp = nltk.RegexpParser(pattern1)\n",
    "        cs = cp.parse(lista)\n",
    "        \n",
    "        iob_tagged = tree2conlltags(cs)\n",
    "            \n",
    "        out_list.append(iob_tagged) \n",
    "        \n",
    "    return out_list\n",
    "\n",
    "tweets['tweet_chunked'] = tweets['tweet_tagged'].apply(chunker)\n",
    "#tweets[tweets.columns[::-1]].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - NER - Reconhecimento de Entidades\n",
    "\n",
    "entrada: tweet_tagged | saída: tweet_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from nltk.chunk.regexp import ChunkString, ChunkRule, ChinkRule \n",
    "from nltk.tree import Tree \n",
    "from contextlib import redirect_stdout\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "def tweet_NER(tweets_list):\n",
    "    \n",
    "    out_list = []\n",
    "    for tweet in tweets_list:\n",
    "        out_list.append(nltk.ne_chunk(tweet))\n",
    "        \n",
    "    return out_list\n",
    "\n",
    "tweets['tweet_NER'] = tweets['tweet_tagged'].apply(chunker)\n",
    "#tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRODUTOS DA ATIVIDADE 01:\n",
    "\n",
    "As entregas da atividade 01 são as colunas do Dataframe com os arrays de tweets:\n",
    "\n",
    "- tweets['tweet_lemma']\n",
    "- tweets['tweet_chunked']\n",
    "- tweets['tweet_NER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "# INICIO ATIVIDADE 02\n",
    "\n",
    "- Extração de BOW-unigrama e bigrama dos tweets\n",
    "- Consulta a bases de conhecimento externas (WordNet, Empath) para definir sinônimos, hiperônimos ou palavras relacionadas com conteúdo ofensivo.\n",
    "- Vetorização dos tweets usando TF_IDF para os unigramas e bigramas\n",
    "- Recuperação dos word-embeddings das palavras de cada tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANDO MÓDULOS NECESSÁRIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download ('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Extração de BOW Unigrama e Bigrama dos Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_grams(words_list, n):\n",
    "     s = []\n",
    "     for ngram in ngrams(words_list, n):\n",
    "         s.append(' '.join(str(i) for i in ngram))\n",
    "     return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIGRAMAS\n",
    "* entrada: tweet_tokens_sw | saída: tweet_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_unigram</th>\n",
       "      <th>tweet_NER</th>\n",
       "      <th>tweet_chunked</th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_sents</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[ask, native, american, take, .]]</td>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>[ she should ask a few native americans what t...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[  go home youre drunk!, 👊🇺🇸👊]</td>\n",
       "      <td>go home youre drunk!      👊🇺🇸👊</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[amazon, investigate, chinese, employee, sell...</td>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>[amazon is investigating chinese employees who...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>[ someone shouldvetaken this piece of shit to ...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[obama, want, liberal, &amp;, amp, ;, illegals, m...</td>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[  obama wanted liberals &amp;amp; illegals to mov...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_unigram  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                           tweet_NER  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                       tweet_chunked  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                         tweet_lemma  \\\n",
       "0                 [[ask, native, american, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigate, chinese, employee, sell...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, want, liberal, &, amp, ;, illegals, m...   \n",
       "\n",
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                         tweet_sents  \\\n",
       "0  [ she should ask a few native americans what t...   \n",
       "1                     [  go home youre drunk!, 👊🇺🇸👊]   \n",
       "2  [amazon is investigating chinese employees who...   \n",
       "3  [ someone shouldvetaken this piece of shit to ...   \n",
       "4  [  obama wanted liberals &amp; illegals to mov...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      👊🇺🇸👊    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigramer(sents_list):\n",
    "    out_list = []\n",
    "    for sent in sents_list:\n",
    "        out_list.append(word_grams(sent,1))\n",
    "    return out_list       \n",
    "    \n",
    "tweets['tweet_unigram'] = tweets['tweet_tokens_sw'].apply(unigramer)\n",
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIGRAMAS\n",
    "* entrada: tweet_tokens_sw | saída: tweet_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_unigram</th>\n",
       "      <th>tweet_bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[ask native, native americans, americans take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[go home, home youre, youre drunk, drunk !], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon investigating, investigating chinese,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone shouldvetaken, shouldvetaken piece, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[[obama wanted, wanted liberals, liberals &amp;, &amp;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_unigram  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                        tweet_bigram  \n",
       "0  [[ask native, native americans, americans take...  \n",
       "1  [[go home, home youre, youre drunk, drunk !], []]  \n",
       "2  [[amazon investigating, investigating chinese,...  \n",
       "3  [[someone shouldvetaken, shouldvetaken piece, ...  \n",
       "4  [[obama wanted, wanted liberals, liberals &, &...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigramer(tokens_list):\n",
    "    out_list = []\n",
    "    for sent in tokens_list:\n",
    "        out_list.append(word_grams(sent,2))\n",
    "    return out_list       \n",
    "    \n",
    "tweets['tweet_bigram'] = tweets['tweet_tokens_sw'].apply(bigramer)\n",
    "#tweets[tweets.columns[::-1]].head()\n",
    "tweets[['tweet_unigram','tweet_bigram']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Uso de bases de conhecimento externas\n",
    "EXTRAÇÃO E ARMAZENAMENTO DE SINONIMOS\n",
    "\n",
    "- Consulta à Base lexico-semântica WordNet : WordNet de Princeton (original em Inglês)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ask', {'take', 'need', 'require', 'enquire', 'inquire', 'ask', 'necessitate', 'involve', 'demand', 'expect', 'postulate', 'call_for'}], ['native', {'indigen', 'aborigine', 'aboriginal', 'indigene', 'native'}], ['american', {'American', 'American_English', 'American_language'}], ['take', {'look_at', 'train', 'direct', 'claim', 'study', 'necessitate', 'ask', 'consume', 'consider', 'postulate', 'use_up', 'contract', 'take_up', 'call_for', 'hold', 'payoff', 'remove', 'get_hold_of', 'withdraw', 'admit', 'guide', 'choose', 'take', 'take_aim', 'lead', 'occupy', 'bring', 'need', 'exact', 'aim', 'hire', 'involve', 'make', 'subscribe_to', 'film', 'issue', 'convey', 'pick_out', 'accept', 'charter', 'engage', 'take_away', 'pack', 'take_on', 'submit', 'ingest', 'shoot', 'conduct', 'read', 'subscribe', 'demand', 'strike', 'return', 'deal', 'take_in', 'learn', 'acquire', 'adopt', 'drive', 'select', 'require', 'lease', 'contain', 'get', 'fill', 'takings', 'yield', 'carry', 'assume', 'proceeds', 'rent', 'have'}], ['.', set()]]\n"
     ]
    }
   ],
   "source": [
    "def synonimer(tokens_list):\n",
    "    out_list = []\n",
    "    synonims = []\n",
    "    lemmas = []\n",
    "    for sent in tokens_list:\n",
    "        for w in sent:\n",
    "            synonyms = wn.synsets(w)\n",
    "            lemmas = set(chain.from_iterable([Word.lemma_names() for Word in synonyms]))\n",
    "            out_list.append([w,lemmas]) #mostra a palavra, e em seguida a lista de sinonimos\n",
    "        return out_list \n",
    "\n",
    "\n",
    "tweets['tweet_sinonimos'] = [synonimer(x) for x in tweets['tweet_lemma']]\n",
    "print(tweets['tweet_sinonimos'][0]) #imprimindo os sinonimos das palavras do primeiro tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_sinonimos</th>\n",
       "      <th>tweet_bigram</th>\n",
       "      <th>tweet_unigram</th>\n",
       "      <th>tweet_NER</th>\n",
       "      <th>tweet_chunked</th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_sents</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[ask, {take, need, require, enquire, inquire,...</td>\n",
       "      <td>[[ask native, native americans, americans take...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[ask, native, american, take, .]]</td>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>[ she should ask a few native americans what t...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[go, {fit, give_way, get_going, spell, give_o...</td>\n",
       "      <td>[[go home, home youre, youre drunk, drunk !], []]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [👊🇺🇸👊]]</td>\n",
       "      <td>[  go home youre drunk!, 👊🇺🇸👊]</td>\n",
       "      <td>go home youre drunk!      👊🇺🇸👊</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[amazon, {Amazon, virago, Amazon_River, amazo...</td>\n",
       "      <td>[[amazon investigating, investigating chinese,...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[amazon, investigate, chinese, employee, sell...</td>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>[amazon is investigating chinese employees who...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[someone, {person, somebody, mortal, someone,...</td>\n",
       "      <td>[[someone shouldvetaken, shouldvetaken piece, ...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>[ someone shouldvetaken this piece of shit to ...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[obama, {}], [want, {privation, need, wishing...</td>\n",
       "      <td>[[obama wanted, wanted liberals, liberals &amp;, &amp;...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[obama, want, liberal, &amp;, amp, ;, illegals, m...</td>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, amp, ;, illegals...</td>\n",
       "      <td>[  obama wanted liberals &amp;amp; illegals to mov...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tweet_sinonimos  \\\n",
       "0  [[ask, {take, need, require, enquire, inquire,...   \n",
       "1  [[go, {fit, give_way, get_going, spell, give_o...   \n",
       "2  [[amazon, {Amazon, virago, Amazon_River, amazo...   \n",
       "3  [[someone, {person, somebody, mortal, someone,...   \n",
       "4  [[obama, {}], [want, {privation, need, wishing...   \n",
       "\n",
       "                                        tweet_bigram  \\\n",
       "0  [[ask native, native americans, americans take...   \n",
       "1  [[go home, home youre, youre drunk, drunk !], []]   \n",
       "2  [[amazon investigating, investigating chinese,...   \n",
       "3  [[someone shouldvetaken, shouldvetaken piece, ...   \n",
       "4  [[obama wanted, wanted liberals, liberals &, &...   \n",
       "\n",
       "                                       tweet_unigram  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                           tweet_NER  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                       tweet_chunked  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                         tweet_lemma  \\\n",
       "0                 [[ask, native, american, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigate, chinese, employee, sell...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, want, liberal, &, amp, ;, illegals, m...   \n",
       "\n",
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1              [[go, home, youre, drunk, !], [👊🇺🇸👊]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, amp, ;, illegals...   \n",
       "\n",
       "                                         tweet_sents  \\\n",
       "0  [ she should ask a few native americans what t...   \n",
       "1                     [  go home youre drunk!, 👊🇺🇸👊]   \n",
       "2  [amazon is investigating chinese employees who...   \n",
       "3  [ someone shouldvetaken this piece of shit to ...   \n",
       "4  [  obama wanted liberals &amp; illegals to mov...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      👊🇺🇸👊    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Vetorização dos tweets usando TF_IDF para os unigramas e bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIGRAMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t0.08870467774061283\n",
      "  (0, 9129)\t0.11349543948027377\n",
      "  (0, 16878)\t0.17903605688560628\n",
      "  (0, 11956)\t0.18346531530338953\n",
      "  (0, 16537)\t0.2913232818613757\n",
      "  (0, 16802)\t0.2365956376126155\n",
      "  (0, 18330)\t0.2067922590837221\n",
      "  (0, 1122)\t0.3446327079544235\n",
      "  (0, 11409)\t0.481427822236437\n",
      "  (0, 6678)\t0.374284281569373\n",
      "  (0, 567)\t0.12606762847148154\n",
      "  (0, 1538)\t0.35758538129472095\n",
      "  (0, 15204)\t0.25257071905846223\n",
      "  (0, 15083)\t0.1781615846241865\n",
      "  (1, 19001)\t0.2539919127450704\n",
      "  (1, 19003)\t0.2545452189205905\n",
      "  (1, 19085)\t0.7406915555768779\n",
      "  (1, 0)\t0.1271936449800965\n",
      "  (1, 5629)\t0.3441724856846966\n",
      "  (1, 18826)\t0.24342893774691327\n",
      "  (1, 8284)\t0.28404122283166106\n",
      "  (1, 7469)\t0.21794694780123144\n"
     ]
    }
   ],
   "source": [
    "tokenizador = TweetTokenizer()\n",
    "\n",
    "cv = TfidfVectorizer(tokenizer = tokenizador.tokenize, ngram_range= (1,1)) #é utilizado o mesmo tokenizador para o processo de vetorização\n",
    "\n",
    "tweets['vetorizacao_unigram'] = cv.fit_transform(tweets['tweet_normalizado'])\n",
    "print(tweets['vetorizacao_unigram'][0][:2]) #print dos TF-IDF do primeiro tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vetorizacao_unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 vetorizacao_unigram\n",
       "0    (0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...\n",
       "1    (0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...\n",
       "2    (0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...\n",
       "3    (0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0...\n",
       "4    (0, 16)\\t0.08870467774061283\\n  (0, 9129)\\t0..."
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[['vetorizacao_unigram']].head() #TF-IDF de cada tweet por unigrama guardado no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Words : \n",
      "          Unigrama         rank\n",
      "11409    oxymoron  6374.104366\n",
      "6678       funner  4955.523888\n",
      "1538          bae  4734.430448\n",
      "1122        apple  4562.937053\n",
      "16537     uniform  3857.120252\n",
      "15204  structural  3344.036320\n",
      "16802       versa  3132.526242\n",
      "11956     pleases  2429.080775\n",
      "16878  villifying  2370.437393\n",
      "15083    stickers  2358.859380\n"
     ]
    }
   ],
   "source": [
    "# Recuperando unigramas  \n",
    "vectorizer = CountVectorizer(ngram_range = (1,1)) \n",
    "X0 = vectorizer.fit_transform(tweets['tweet_normalizado'])  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "\n",
    "# TOP RANKING features \n",
    "sums = tweets['vetorizacao_unigram'].sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0, col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['Unigrama', 'rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "print (\"\\n\\nWords : \\n\", words.head(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIGRAMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t0.08870467774061283\n",
      "  (0, 9129)\t0.11349543948027377\n",
      "  (0, 16878)\t0.17903605688560628\n",
      "  (0, 11956)\t0.18346531530338953\n",
      "  (0, 16537)\t0.2913232818613757\n",
      "  (0, 16802)\t0.2365956376126155\n",
      "  (0, 18330)\t0.2067922590837221\n",
      "  (0, 1122)\t0.3446327079544235\n",
      "  (0, 11409)\t0.481427822236437\n",
      "  (0, 6678)\t0.374284281569373\n",
      "  (0, 567)\t0.12606762847148154\n",
      "  (0, 1538)\t0.35758538129472095\n",
      "  (0, 15204)\t0.25257071905846223\n",
      "  (0, 15083)\t0.1781615846241865\n",
      "  (1, 19001)\t0.2539919127450704\n",
      "  (1, 19003)\t0.2545452189205905\n",
      "  (1, 19085)\t0.7406915555768779\n",
      "  (1, 0)\t0.1271936449800965\n",
      "  (1, 5629)\t0.3441724856846966\n",
      "  (1, 18826)\t0.24342893774691327\n",
      "  (1, 8284)\t0.28404122283166106\n",
      "  (1, 7469)\t0.21794694780123144\n"
     ]
    }
   ],
   "source": [
    "tokenizador = TweetTokenizer()\n",
    "\n",
    "cv2 = TfidfVectorizer(tokenizer = tokenizador.tokenize, ngram_range= (2,2))\n",
    "\n",
    "tweets['vetorizacao_bigram'] = cv2.fit_transform(tweets['tweet_normalizado'])\n",
    "\n",
    "print(tweets['vetorizacao_unigram'][0][:2]) #print dos TF-IDF do primeiro tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vetorizacao_bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 59169)\\t0.179042155839197\\n  (0, 108541)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 59169)\\t0.179042155839197\\n  (0, 108541)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 59169)\\t0.179042155839197\\n  (0, 108541)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 59169)\\t0.179042155839197\\n  (0, 108541)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 59169)\\t0.179042155839197\\n  (0, 108541)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  vetorizacao_bigram\n",
       "0    (0, 59169)\\t0.179042155839197\\n  (0, 108541)...\n",
       "1    (0, 59169)\\t0.179042155839197\\n  (0, 108541)...\n",
       "2    (0, 59169)\\t0.179042155839197\\n  (0, 108541)...\n",
       "3    (0, 59169)\\t0.179042155839197\\n  (0, 108541)...\n",
       "4    (0, 59169)\\t0.179042155839197\\n  (0, 108541)..."
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[['vetorizacao_bigram']].head() #TF-IDF de cada tweet por bigrama guardado no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Words : \n",
      "                   Bigrama         rank\n",
      "13365          basic this  4360.666277\n",
      "42948        had fraction  4360.666277\n",
      "106546            too hot  4360.666277\n",
      "73811           one hopes  4180.209924\n",
      "19113           by antifa  4180.209924\n",
      "118670             you be  4052.174104\n",
      "94704        suck liberal  3952.861806\n",
      "100999  their corruptions  3743.681931\n",
      "94005            story as  3038.653235\n",
      "8139      another russian  3016.395590\n"
     ]
    }
   ],
   "source": [
    "# Recuperando bigramas  \n",
    "vectorizer = CountVectorizer(ngram_range = (2,2)) \n",
    "X1 = vectorizer.fit_transform(tweets['tweet_normalizado'])  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "\n",
    "# TOP RANKING features \n",
    "sums = tweets['vetorizacao_bigram'].sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0, col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['Bigrama', 'rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "print (\"\\n\\nWords : \\n\", words.head(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Recuperação dos word-embeddings das palavras de cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'home', 'youre', 'drunk', '!', '👊🇺🇸👊']\n",
      "[-0.00958302 -0.00091067  0.00135524 -0.00891763 -0.0073118  -0.00554992\n",
      " -0.0029997  -0.00482254 -0.00358181  0.00396424 -0.00811621 -0.00501202\n",
      " -0.00404956 -0.00286173  0.00822034 -0.00489247  0.00417566  0.00926429\n",
      " -0.00232018 -0.0072652  -0.00718071  0.00045939 -0.00106482  0.0060225\n",
      " -0.0070861   0.00968049  0.0085284  -0.00224672 -0.00287142 -0.00967177\n",
      "  0.00643119  0.00042172 -0.00571959  0.00372656  0.0052601  -0.00831407\n",
      "  0.00839414  0.00742136  0.00053315 -0.00252774 -0.00917053 -0.00252628\n",
      " -0.00150322 -0.00480463  0.00031526  0.00639866  0.0071882   0.00936461\n",
      " -0.00595977  0.00864219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#PARA UM TWEET DE CADA VEZ\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "sent = tweets['tweet_tokens_sw'][1]\n",
    "model = Word2Vec(sent, min_count=1,size=50,workers=3,window=2,sg=1)\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "print(model['go'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TENTANDO PEGAR DE TODOS OS TWEETS E ARMAZENAR EM UMA ESTRUTURA\n",
    "# AINDA SEM SUCESSO\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "def embedder(tweets_list):\n",
    "    out_list = []\n",
    "    for x in range(0,len(tweets_list)):\n",
    "        sent = tweets_list[x]\n",
    "        model = Word2Vec(sent, min_count=1,size=50,workers=3,window=2,sg=1)\n",
    "        out_list.append([words,model])\n",
    "    return out_list\n",
    "    \n",
    "\n",
    "tweets['word_embeddings'] = tweets['tweet_tokens_sw'].apply(embedder)    \n",
    "tweets[['vetorizacao_bigram']].head()\n",
    "#print(words)\n",
    "#print(model['home'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRODUTOS DA ATIVIDADE 02:\n",
    "\n",
    "As entregas da atividade 01 são as colunas do Dataframe com os arrays de tweets:\n",
    "\n",
    "- tweets['tweet_unigram']\n",
    "- tweets['tweet_bigram']\n",
    "- tweets['vetorizacao_unigram']\n",
    "- tweets['vetorizacao_bigram']\n",
    "- Por enquando, somente conseguimos recuperar as word-embeddings das palavras de um tweet por vez, com os códigos do item 12."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
